MPI POLYNOMIAL MULTIPLICATION LAB - DOCUMENTATION
=================================================

1. ALGORITHM DESCRIPTIONS

A. Naive Multiplication (O(n^2))
   The Naive algorithm performs a standard polynomial multiplication (convolution).
   For two polynomials A (size n) and B (size m), the result polynomial C has a 
   size of n + m - 1.
   
   - Logic: Each coefficient C[k] is calculated by summing A[i] * B[j] for all 
     pairs where i + j = k.
   - Implementation: Two nested loops iterate through the input vectors.

B. Karatsuba Multiplication (O(n^1.58))
   The Karatsuba algorithm uses a Divide-and-Conquer approach to reduce the number 
   of multiplicative operations.
   
   - Splitting: The inputs A and B are split into lower halves (A0, B0) and upper 
     halves (A1, B1) at index k = n/2.
   - Recursion: It computes three partial products:
     1. P1 = A0 * B0
     2. P2 = A1 * B1
     3. P3 = (A0 + A1) * (B0 + B1)
   - Combination: The middle term is derived as (P3 - P1 - P2), avoiding a fourth 
     multiplication.


2. DISTRIBUTION AND COMMUNICATION STRATEGIES (MPI)

A. Naive MPI (Data Decomposition)
   - Strategy: Work Sharing (Output Partitioning).
   - Distribution:
     The Master node (Rank 0) uses `MPI_Bcast` to send the full input polynomials 
     A and B to all worker nodes. This ensures every node has the necessary data 
     to compute any part of the result.
   - Partitioning:
     The result vector indices (0 to n+m-2) are divided equally among the 
     available MPI processes. Each rank calculates its own unique `start` and 
     `end` index range.
   - Communication:
     1. Each node computes its assigned partial range of the result polynomial in 
        a local buffer.
     2. The Master node collects these partial results using `MPI_Gatherv`, which 
        handles potentially uneven chunk sizes, stitching them together into the 
        final result vector.

B. Karatsuba MPI (Task Parallelism)
   - Strategy: Manager-Worker (Top-Level Split).
   - Logic: 
     Parallelism is applied only at the first level of the recursion tree to 
     minimize communication overhead.
   - Task Assignment:
     Rank 0 (Manager): Prepares sub-problems by splitting inputs and calculating 
     sums (A0+A1, B0+B1). It then dispatches work:
       - Rank 1: Receives A0, B0 via `MPI_Send`. Computes P1.
       - Rank 2: Receives A1, B1 via `MPI_Send`. Computes P2.
       - Rank 3: Receives SumA, SumB via `MPI_Send`. Computes P3.
   - Execution: 
     Worker nodes receive their payload and perform the multiplication using the 
     optimized Sequential Karatsuba algorithm (from Lab 5).
   - Synchronization: 
     Rank 0 uses `MPI_Recv` to wait for the three results. Once received, it 
     performs the final linear combination (O(n)) to construct the result.
   - Fallback:
     If the number of MPI processes is less than 4, the algorithm reverts to 
     sequential execution on Rank 0.


3. PERFORMANCE MEASUREMENTS

Scenario 1: N = 10,000
   1. MPI Naive:       0.481274 s
   2. MPI Karatsuba:   0.0981233 s
   3. Seq Naive:       0.899447 s
   3. Seq Karatsuba:   0.134822 s

Scenario 2: N = 50,000
   1. MPI Naive:       11.7155 s
   2. MPI Karatsuba:   1.12471 s
   3. Seq Naive:       22.7394 s
   3. Seq Karatsuba:   1.68785 s

Scenario 3: N = 100,000
   1. MPI Naive:       55.3869 s
   2. MPI Karatsuba:   4.98357 s
   3. Seq Naive:       97.7688 s
   3. Seq Karatsuba:   5.22665 s
